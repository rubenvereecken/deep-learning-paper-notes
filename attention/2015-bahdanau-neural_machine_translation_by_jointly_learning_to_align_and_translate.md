# Neural Machine Translation By Jointly Learning To Align and Translate

Bahdanau, D., Cho, K., & Bengio, Y. (2015). Neural Machine Translation By Jointly Learning To Align and Translate. Iclr 2015, 1–15. https://doi.org/10.1146/annurev.neuro.26.041002.131047

https://arxiv.org/abs/1409.0473


## Notes

- Original, often-referenced NMT paper using “soft” attention
- “Soft-searches” for words in the input sentence while generating target words. This part is alignment
- The probability for a word (the output) is conditioned on an RNN hidden state and a context vector c which is a summary of the alignment.
- The context vector is a sum of the inputs, weighted by attention
- Inputs to this alignment model are actually annotations generated by an encoder
- Attention is the likelihood for each input element that it is relevant for generating the current output. It depends on a score, this is the alignment model. In this paper, “[they] parametrize the alignment model a as a feedforward neural network which is jointly trained with all the other components of the proposed system.”

