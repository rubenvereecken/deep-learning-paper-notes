# Attention Is All You Need

https://arxiv.org/abs/1706.03762

## TLDR
Introduces the _Transformer_ architecture,
a sequence-to-sequence neural network
that relies entirely on attention,
without recurrence or convolutions.

## Notes

## Thoughts
