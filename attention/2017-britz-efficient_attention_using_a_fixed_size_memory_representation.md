# Efficient Attention using a Fixed-Size Memory Representation

https://arxiv.org/abs/1707.00110

## TLDR

## Notes
- Learn _K_ attention context vectors while reading source
  thus avoiding looking back at the source while decoding.
  During each step of decoding
  use a weighted average of these context vectors.

## Thoughts
