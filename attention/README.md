# Attention

Attention-inspired mechanisms
have mostly started with an interest in
Neural Machine Translation.
Still relevant is the paper by
[Bahdanau et al.](2015_bahdanau_neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
who introduced soft attention
which is still being used
in barely altered variations.
